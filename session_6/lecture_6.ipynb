{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 6:\n",
    "## Machine learning for causal inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. [A general problem](#A-general-problem)\n",
    "1. [Inference with Lasso](#Inference-with-Lasso)\n",
    "1. [Double Machine Learning](#Double-Machine-Learning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A general problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimate treatment effect with selection\n",
    "\n",
    "Often we are interested in estimating average treatment effect (ATE) of $T$ on $y$ in observational data.\n",
    "\n",
    "- Matching has for a long time been the defacto standard \n",
    "  - (assuming we measure enough variables to assignment conditionally random)\n",
    "- Can we somehow solve the problem using machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimate treatment effect with selection\n",
    "[Chernozhukov et al. (2018)](http://economics.mit.edu/files/12538) assume that we have the following data generating process\n",
    "\\begin{align}\n",
    "y=& T\\theta_0+g_0(X)+U\\\\\n",
    "T=&m_0(X) + V\\\\\n",
    "E[U|X,T]=&0\\\\\n",
    "E[V|X]=&0\\\\\n",
    "\\end{align}\n",
    "\n",
    "Basic model properties:\n",
    "- The outcome $y$ is confounded by unknown nuisance function $g_0(\\cdot)$ \n",
    "- The treatment $T$ suffer from selection on observable, where $m_0$ is  unknown propensity function\n",
    "  - Note assumed no selection on unobservables (only \"mild\" econometric problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Estimate treatment effect with selection\n",
    "\n",
    "There have been multiple ways proposed to solve the problem:\n",
    "- *Directly* modify ML to allow for estimation of treatment effect\n",
    "- *Indirectly* modify estimation procedure to incorporate ML \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Inference with Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Treatment effects in linear models \n",
    "\n",
    "Suppose we want to estimate a linear model parameter for the causal effect of treatment $T_i$ on $y_i$. \n",
    "\n",
    "\\begin{equation}y_i=\\alpha T_i+x_i\\beta+r_{yi}+\\zeta_i\\end{equation}\n",
    "\n",
    "- We follow notation in [Belloni et al., 2015](https://doi.org/10.1257/jep.28.2.29)\n",
    "- We let $r_{yi}$ be an approximation error (we don't know the functional form)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Treatment effects in linear models (2)\n",
    "\n",
    "How to select model, i.e. subset of $x$?\n",
    "\n",
    "- Classic econometrics: \n",
    "  - Use **OLS** and include covariates based on theory or inference\n",
    "  - Problem how to delete covariates systematically? Adjust for multiple hypothesis testing?\n",
    "  \n",
    "- Machine learning:\n",
    "  - Use **LASSO** to perform covariate selection\n",
    "  - Note - estimates are biased towards zero!  \n",
    "    - Problem we omit potentially relevant variables!!\n",
    "    - LASSO excludes  possible confounders if little predictive power $y_i$. \n",
    "    - Excluded variables may still have an effect through $T_i$, e.g. covariates correlated with treatment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing the LASSO\n",
    "\n",
    "A simple solution suggested by [Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29) us to use Post-LASSO to correct for bias:\n",
    "- Step 1: estimate two LASSO models\n",
    "    - a) Regress $y_i$ on $x_i$ \n",
    "    - b) Regress $T_i$ on $x_i$ \n",
    "- Step 2: run OLS using only variables that were kept in either LASSOs\n",
    "\n",
    "What about inference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need further assumptions on sparsity.\n",
    "- See [Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) online appendix for details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing the LASSO (2)\n",
    "The LASSO picks the correct variables even if there are more variables than observations!\n",
    "- In high dimensions this does not work - can return more variables to use estimate than possible in OLS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Double Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear DML\n",
    "\n",
    "[Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29), [Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) write down the two \"prediction equations\":\n",
    "\n",
    "\\begin{align}\n",
    "y_i=&\\alpha T_i+x_i^{\\prime}\\theta_y+r_{yi}+\\zeta_i\\\\\n",
    "T_{i}=&x_{i}^{\\prime} \\theta_{t}+r_{t i}+v_{i}\n",
    "\\end{align}\n",
    "\n",
    "The two equations can be combined into a single structural equation (substite $T_i$ into $y_i$):\n",
    "\n",
    "\\begin{align}y_{i}=&x_{i}^{\\prime}\\left(\\alpha \\theta_{t}+\\theta_{y}\\right)+\\left(\\alpha r_{t i}+r_{y i}\\right)+\\left(\\alpha v_{i}+\\zeta_{i}\\right)\\\\=&x_{i}^{\\prime} \\pi+r_{c i}+\\varepsilon_{i}\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear DML (2)\n",
    "\n",
    "[Chernozhukov et al. (2015)](https://doi.org/10.1257/aer.p20151022) states the following algorithm for :\n",
    "\n",
    "1. run the two LASSO equations (as in POST-LASSO) and obtain residuals \n",
    "  - $\\hat{\\rho}_i^y$ from $y_i$ on $x_i$\n",
    "  - $\\hat{\\rho}_i^d$ from $T_i$ on $x_i$\n",
    "1. run a regression of $\\hat{\\rho}_i^y$ on $\\hat{\\rho}_i^d$\n",
    "\n",
    "What is the intuition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Similar to Frisch-Waugh-Lowell where we partial out effects.\n",
    "  - We partial out effect of $x_i$ on both $y_i$ and on $T_i$ seperately \n",
    "- Innovation: We make double selection of variables using LASSO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear DML (3)\n",
    "\n",
    "[Belloni et al. (2015)](https://doi.org/10.1257/jep.28.2.29) simulates the performance of post selection estimators\n",
    "\n",
    "<center><img src='beh2014_fig1.JPG' alt=\"Drawing\" style=\"width: 800px;\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive solution\n",
    "\n",
    "What happens when use machine learning estimator to estimate directly estimate in $\\theta_0$ and $g_0(\\cdot)$ in $y=T\\theta_0+g_0(X)$ to control for confounders? \n",
    "- Where we use a an auxiliary subsample $I^c$ to compute $\\hat{g}_0(\\cdot)$ using a possibly non-linear model.\n",
    "- Assume subsample is half of the sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\hat{\\theta}_0=\\frac{\\frac{1}{n}\\sum_{i\\in I}T_i(y_i-\\hat{g}_0(X_i))}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive solution\n",
    "\n",
    "We decompose estimator into scaled estimation error\n",
    "\n",
    "$$\\sqrt{n}(\\theta-\\hat{\\theta}_0)=\n",
    "\\frac{\\frac{1}{\\sqrt{n}}\\sum_{i\\in I}T_iU_i}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}+\\frac{\\frac{1}{\\sqrt{n}}\\sum_{i\\in I}T_i(g_0-\\hat{g}_0(X_i))}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}\n",
    "$$\n",
    "\n",
    "What could be problematic here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive problem\n",
    "\n",
    "Issue is that $\\hat{g}$ will be systematically biased as we are curbing overfitting, e.g. through regularization. \n",
    "- Same problem arises for tree-based and neural network models.\n",
    "- Estimator will have bias term that diverges and is not centered:\n",
    "\n",
    "$$\\frac{\\frac{1}{\\sqrt{n}}\\sum_{i\\in I}T_i(g_0-\\hat{g}_0(X_i))}{E[T_i^2]}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonalization\n",
    "\n",
    "Suppose we also estimate $\\hat{m}_0(\\cdot)$ on the auxiliary sample $I^c$. We can then make the following estimate:\n",
    "\n",
    "$$\\check{\\theta}_0=\\frac{\\frac{1}{n}\\sum_{i\\in I}\\hat{V}_i(y_i-\\hat{g}_0(X_i))}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}$$\n",
    "\n",
    "where we use the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonalization\n",
    "\n",
    "We decompose estimator into scaled estimation error\n",
    "\n",
    "$$\\sqrt{n}(\\theta-\\hat{\\theta}_0)=\n",
    "\\frac{\\frac{1}{\\sqrt{n}}\\sum_{i\\in I}T_iU_i}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}+\\frac{\\frac{1}{\\sqrt{n}}\\sum_{i\\in I}(m_0(X_i)-\\hat{m}_0(X_i))(g_0-\\hat{g}_0(X_i))}{\\frac{1}{n}\\sum_{i\\in I}T_i^2}\n",
    "$$\n",
    "\n",
    "This solves the problem as the product of estimation errors vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orthogonalization\n",
    "\n",
    "The first major contribution of [Chernozhukov et al. (2018)](http://economics.mit.edu/files/12538) is to show that in general the second double debiasing procedure leads to consistent estimates and can be used estimate average treatment effects.\n",
    "\n",
    "- The proof depends on sample splitting - using an independent auxiliary sample for estimating $\\hat{m}_0,\\hat{g}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation details\n",
    "\n",
    "Problem - what to do with auxiliary sample? \n",
    "\n",
    "To gain efficient estimates [Chernozhukov et al. (2018)](http://economics.mit.edu/files/12538) \n",
    "- Problem - what to do with auxiliary sample?\n",
    "- We rotate sample using **cross-fitting**: first use one part as auxiliary sample, then the other. Like cross-validation in supervised ML.\n",
    "- This is second major contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation details\n",
    "\n",
    "How do we estimate $\\hat{m}_0,\\hat{g}_0$? This can be done using cross-validation on auxiliary sample $I^c$. Available estimators:\n",
    "\n",
    "- linear/logistic models, including regularized\n",
    "- tree based inclding random forests, boosted trees\n",
    "- neural networks \n",
    "- kernel models including suppert vector machine \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "The DML approach is extended in the paper to:\n",
    "- compute Local Average Treatment Effects (LATE) \n",
    "- compute Instrumental Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The end\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
